{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from config import dbuser, dbpassword, dbhost, dbport, dbname\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING A DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A HEROKU DATASET\n",
    "# Heroku only takes data with <10k rows so I created a heroku dataset (incase we ever want to host on heroku).\n",
    "\n",
    "def import_func(country_code):\n",
    "    \n",
    "    # Creating Path\n",
    "    path = os.path.join('data','newData',f'{country_code}_youtube_trending_data.csv')\n",
    "    \n",
    "    # Storing dataframe to df\n",
    "    dfh=pd.read_csv(path)\n",
    "    \n",
    "    # Removing unwanted columns below\n",
    "    dfh=dfh[['video_id','title','publishedAt','channelTitle','categoryId','trending_date','view_count','likes','dislikes','comment_count','thumbnail_link']]\n",
    "    \n",
    "    # Renaming columns\n",
    "    dfh = dfh.rename(columns={'view_count': 'views', 'likes': 'likes', 'dislikes': 'dislikes', 'comment_count': 'comments'})\n",
    "    \n",
    "    # Changing object types to date types for two columns\n",
    "    dfh['publishedAt']=pd.to_datetime(dfh['publishedAt'])\n",
    "    dfh['trending_date']=pd.to_datetime(dfh['trending_date'])\n",
    "    \n",
    "    # Removing time stamp from date\n",
    "    dfh['publishedAt']=dfh['publishedAt'].dt.date \n",
    "    dfh['trending_date']=dfh['trending_date'].dt.date\n",
    "    \n",
    "    # For loop for each csv file\n",
    "    with open(f'data/newData/{country_code}_category_id.json', 'r') as read_file:\n",
    "        category_ids = json.load(read_file)\n",
    "\n",
    "        dfh=dfh.astype({'categoryId': 'str'})\n",
    "        \n",
    "    for index,row in dfh.iterrows():\n",
    "    \n",
    "        for entry in category_ids[\"items\"]:\n",
    "\n",
    "            if row[\"categoryId\"]==entry[\"id\"]:\n",
    "                dfh.at[index,\"categoryId\"]=entry[\"snippet\"][\"title\"]\n",
    "    \n",
    "    # Select 999 rows from each country in dataframe (so we end up with <10k rows ~1000 rows per country * 10 countries)\n",
    "    dfh = dfh.nlargest(999, 'views')\n",
    "    \n",
    "    # Adding country Code as column\n",
    "    dfh['country']=f'{country_code}'\n",
    "    col_name='country'\n",
    "    \n",
    "    # Moving country code to first column\n",
    "    first_col = dfh.pop(col_name)\n",
    "    dfh.insert(0,col_name,first_col)\n",
    "    \n",
    "    return dfh\n",
    "\n",
    "# USA Dataframe\n",
    "dfh_us=import_func('US')\n",
    "\n",
    "# Brasil Dataframe\n",
    "dfh_br=import_func('BR')\n",
    "\n",
    "# Canada Dataframe\n",
    "dfh_ca=import_func('CA')\n",
    "\n",
    "# Mexico Dataframe\n",
    "dfh_mx=import_func('MX')\n",
    "\n",
    "# GB Dataframe\n",
    "dfh_gb=import_func('GB')\n",
    "\n",
    "# France Dataframe\n",
    "dfh_fr=import_func('FR')\n",
    "\n",
    "# Russia Dataframe\n",
    "dfh_ru=import_func('RU')\n",
    "\n",
    "# Japan Dataframe\n",
    "dfh_jp=import_func('JP')\n",
    "\n",
    "# Korea Dataframe\n",
    "dfh_kr=import_func('KR')\n",
    "\n",
    "# India Dataframe\n",
    "dfh_in=import_func('IN')\n",
    "\n",
    "\n",
    "# Creating a varible to add all dfs\n",
    "country_dfh=[dfh_us, dfh_br, dfh_ca, dfh_mx, dfh_gb, dfh_fr, dfh_ru ,dfh_jp, dfh_kr ,dfh_in]\n",
    "\n",
    "# Merge output into one table\n",
    "dfh_main = pd.concat(country_dfh)\n",
    "dfh_main\n",
    "\n",
    "# Fix the 29 vs Non profits issue\n",
    "dfh_main['categoryId'] = dfh_main['categoryId'].replace([\"29\"],\"Nonprofits & Activism\")\n",
    "\n",
    "# View output\n",
    "dfh_main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECTING AND LOADING DATA INTO THE DATABASE FOR HEROKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UPLOAD DATASET TO DATABASE (POSTGRESS SQL DB)\n",
    "# You first have to go create the db and then use the same name as the db you have created\n",
    "# (named youtube_table_v1) in postgres\n",
    "\n",
    "# # 1 Connect to Database (with Local db and all)\n",
    "# pg_user = 'postgres'\n",
    "# pg_password=password\n",
    "# db_name = 'youtube_database'\n",
    "# connection_string = f'{pg_user}:{password}@localhost:5432/{db_name}'\n",
    "# engine=create_engine(f'postgresql://{connection_string}')\n",
    "\n",
    "# 1 Connect to Database (Alternative with AWS db and all)\n",
    "dbuser = 'postgres'\n",
    "dbpassword = 'Sm6Jc5bqbiNQdsVAo7eN'\n",
    "dbhost = 'localhost'\n",
    "dbport = '5432'\n",
    "dbname= 'YTP_database'\n",
    "connection_string2 = f'{dbuser}:{dbpassword}@database-1.cvmfiiilpm7y.us-east-1.rds.amazonaws.com:{dbport}/{dbname}'\n",
    "engine=create_engine(f'postgresql://{connection_string2}')\n",
    "\n",
    "# 2 Upload to postgres\n",
    "dfh_main.to_sql(name='youtube_table_v1',con=engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
